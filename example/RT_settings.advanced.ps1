### Тут перечислены дополнительные параметры, которые можно использовать при использовании в разных вариациях скрипта

# Если у вас несколько клиентов, то их можно перечислить тут
# Нужный скрипт вызывать с аргуметом явно указывающим на клиент
# -UsedClient myqbit
$client_list = @(
    [ordered]@{
        name     = 'myqbit'
        type     = 'qbittorrent'
        url      = 'http://192.168.0.1:9101'
        login    = 'admin'
        password = 'admin'
        # При включённой опции, backuper не будет перебирать все раздачи в клиенте, а только те, которые найдены в файле с заверщёнными
        hashes_only = 1
    }
    [ordered]@{
        name     = 'mytrans'
        type     = 'transmission'
        url      = 'http://192.168.0.1:9102'
        login    = 'admin'
        password = 'admin'
    }
)

# Дополниельные настройки, для гугл дисков.
$google_params = @{
    # [number] Фактическое количество используемых аккаунтов
    # для windows - количество подключённых дисков, должно быть равно колву калаогов указанных в folders
    # для linux - количество аккаунтов подключённых через rclone
    accounts_count = 1


    # [number] Используемое количество uploader-ов, т.е. потоков выгрузки.
    # При желании можно запускать несколько версий uploader, каждый из которых будет обрабатывать только "свои" архивы чередуя их.
    uploaders_count = 1

    ### Для linux актуально, ТОЛЬКО ЕСЛИ вы используете rclone с кешем !!! ["vfs-cache-mode writes|full"] и задан путь ["cache-dir /rclone-cache"]
    # # Путь к кешу гугл диска и размер, превышение которого нежелательно.
    cache = '/rclone-cache'
    cache_size = 60gb

    # Время "устаревания" списков архивов.
    decay_hours = 18
}


## Параметры связанные с ахривацией [backuper]
$backuper = @{
    # ссылка на архиватор 7z
    p7z = '7z'

    # [0/1] Скрывать ли вывод процесса архивации/проверки
    h7z = 0

    # [number] Колво ядер процессора, которые используются в архивировании
    cores = 1

    # [0/1] Если включено, то backuper НЕ БУДЕТ искать раздачи по всему клиенту. Будет использован только файл hashes.txt.
    hashes_only = 0
    # [number] Кол-во хешей, забираемых из файла за раз.
    hashes_step = 30

    # Путь к каталогу хранения архивов
    zip_folder = '/data/backuper'
    # При использовании backuper и uploader раздельно, имеет смысл ограничить размер промежуточной папки с архивами
    zip_folder_size = 100gb

    # [number] Степень сжатия архива по умолчанию (0-без сжатия)
    compression = 1
    # Степень сжатия архива в зависимости от подраздела. Уже сжатый контент (видео, архивы) - сжимать смысла нет.
    # от 0 до 9, если не указано - то берётся из предыдущей настройки. 0 - без сжатия, 9 - максимальное сжатие
    sections_compression = @{
        110  = 0 # сериалы
        235  = 0 # сериалы
        266  = 0 # сериалы
        594  = 0 # сериалы
        704  = 0 # сериалы
        1105 = 0 # аниме
        1258 = 0 # видео
        2404 = 0 # сериалы
    }
}


## Параметры связанные с выгрузкой в гугл и последующей очисткой [uploader + cleaner]
$uploader = @{
    validate = 1             # [0/1] Проверять ли архив перед загрузкой в гугл.
    delete   = 0             # [0/1] Удалять ли раздачу из клиента, после архивирования.
    delete_category = 'temp' # Категория раздачи в клиенте, проверяемая при удалении раздач после архивирования.
}


# [0/1] Используемые модули. Каждый следует включить, если планируете использовать.
# Например, если у вас несколько клиентов, и канал интернета позволяет, можно запустить разны епроцессы для разных целей
 # N процессов backuper будут архивировать раздачи в общую папку $backuper.zip_folder. Каждый из своего клиента
 # uploader будет брать архивы из общей папки и заливать в облако. Можно использовать несколько, если канал позволяет.
 # cleaner будет удалять раздачи и всех клиентов, если включены соответствующие опции в $uploader
$used_modules = @{
    # Только архивирование раздачи перенос в каталог finished. Можно запустить несколько штук, по одной на клиента
    backuper = 0
    # Процесс сбора архивов из finished и перенос их в гугл
    uploader = 0
    # Процесс удаления из клиента раздач, архивирование и залитие в гугл которых, прошло успешно
    cleaner  = 0
}

# Настроки для collector
$collector = @{
    # Категория, которая будет присвоена новым раздачам
    category = 'temp'

    # каталог, в который скачивать раздачи
    collect      = '/data/downloads'
    # Лимит суммарнно занятого объёма раздачами с указанной категорией
    collect_size = 50gb
    # Лимит общего количества раздач с указанной категорией
    collect_count = 300

    # [0/1] cоздавать ли подкаталоги по ID для раздач, которые добавит коллектор. 1 - создавать, 0 - не надо
    sub_folder = 1
}

# Настройки для restorator
$restorator = @{
    # Категория, которая будет присвоена восстановленным раздачам
    category = 'restore'

    # каталог для восстановления из архивов
    path = 'K:\Хранимое\Прочее'  # '/data/restore'
    # количество дней, за которые подбирать запросы на восстановление
    look_behind_days = 1
    # количество дней сидирования после восстановления
    keep_seeding_days = 3

    # максимальный размер одной восстанавливаемой раздачи
    max_size = 10gb # 10 Гб

    # [0/1] cоздавать ли подкаталоги по ID для раздач при восстановлении из архивов. 1 - создавать, 0 - не надо
    sub_folder = 1
}


# Для использования скриптов collector или restorator нужны данные для авторизации на форуме
# Следует заполнить этот блок. Проксю можно взять ту, которую используете для WebTLO
$forum = @{
    login    = 'forum_login'    # Логин для форума
    password = 'forum_password' # Пароль для форума

    proxy          = 1          # использовать ли прокси. Если у вас ВПН или форум не блокируется - можно выключить.
    proxy_address  = 'type://url:port'
    proxy_login    = 'proxy_login'
    proxy_password = 'proxy_password'
}


